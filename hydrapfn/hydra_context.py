from typing import Optional

import torch
from torch import nn, Tensor

from mamba_ssm.ops.triton.layer_norm import RMSNorm, layer_norm_fn

from torch import nn
import math
import torch
import copy
from functools import partial
from mamba_ssm.modules.mamba_simple import Mamba
from hydra.modules.hydra import Hydra
from mamba_ssm.modules.mamba2 import Mamba2
from mamba_ssm.modules.block import Block
from mamba_ssm.modules.mlp import GatedMLP
from mamba_ssm.modules.mha import MHA

try:
    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn
except ImportError:
    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None



#
# Copied the Block from the official Mamba repository here and edited it for Hydra. Hydra does not have inference params so they are not included.
#
class HydraBlock(nn.Module):
    def __init__(
        self, dim, mixer_cls, mlp_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False
    ):
        """
        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection"

        This Block has a slightly different structure compared to a regular
        prenorm Transformer block.
        The standard block is: LN -> MHA/MLP -> Add.
        [Ref: https://arxiv.org/abs/2002.04745]
        Here we have: Add -> LN -> Mixer, returning both
        the hidden_states (output of the mixer) and the residual.
        This is purely for performance reasons, as we can fuse add and LayerNorm.
        The residual needs to be provided (except for the very first block).
        """
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.norm = norm_cls(dim)
        self.mixer = mixer_cls(dim)
        if mlp_cls is not nn.Identity:
            self.norm2 = norm_cls(dim)
            self.mlp = mlp_cls(dim)
        else:
            self.mlp = None
        if self.fused_add_norm:
            assert RMSNorm is not None, "RMSNorm import fails"
            assert isinstance(
                self.norm, (nn.LayerNorm, RMSNorm)
            ), "Only LayerNorm and RMSNorm are supported for fused_add_norm"

    def forward(
            self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None, **mixer_kwargs
    ):
        r"""Pass the input through the encoder layer.

        Args:
            hidden_states: the sequence to the encoder layer (required).
            residual: hidden_states = Mixer(LN(residual))
        """
        if not self.fused_add_norm:
            residual = (hidden_states + residual) if residual is not None else hidden_states
            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))
            if self.residual_in_fp32:
                residual = residual.to(torch.float32)
        else:
            hidden_states, residual = layer_norm_fn(
                hidden_states,
                self.norm.weight,
                self.norm.bias,
                residual=residual,
                prenorm=True,
                residual_in_fp32=self.residual_in_fp32,
                eps=self.norm.eps,
                is_rms_norm=isinstance(self.norm, RMSNorm)
            )
        hidden_states = self.mixer(hidden_states, **mixer_kwargs)

        if self.mlp is not None:
            if not self.fused_add_norm:
                residual = hidden_states + residual
                residual = self.norm2(residual.to(dtype=self.norm2.weight.dtype))
                if self.residual_in_fp32:
                    residual = residual.to(torch.float32)
            else:
                hidden_states, residual = layer_norm_fn(
                    hidden_states,
                    self.norm2.weight,
                    self.norm2.bias,
                    residual=residual,
                    prenorm=True,
                    residual_in_fp32=self.residual_in_fp32,
                    eps=self.norm2.eps,
                    is_rms_norm=isinstance(self.norm2, RMSNorm)
                )
            hidden_states = self.mlp(hidden_states)

        return hidden_states, residual

    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):
        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)
    


def create_block(
    d_model,
    d_intermediate,
    ssm_cfg=None,
    attn_layer_idx=None,
    attn_cfg=None,
    norm_epsilon=1e-5,
    rms_norm=False,
    residual_in_fp32=False,
    fused_add_norm=False,
    layer_idx=None,
    device=None,
    dtype=None,
):
    '''
    Function from https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py - Here used to implement hydra.
    '''

    if ssm_cfg is None:
        ssm_cfg = {}
    if attn_layer_idx is None:
        attn_layer_idx = []
    if attn_cfg is None:
        attn_cfg = {}
    factory_kwargs = {"device": device, "dtype": dtype}
    if layer_idx not in attn_layer_idx:

        #print(f"SSM cfg: {ssm_cfg}")
        # Create a copy of the config to modify
        ssm_cfg = copy.deepcopy(ssm_cfg) if ssm_cfg is not None else {}
        mixer_cls = partial(
            Hydra,
            layer_idx=layer_idx,
            #**ssm_cfg,
            **factory_kwargs
        )
    else:
        mixer_cls = partial(MHA, layer_idx=layer_idx, **attn_cfg, **factory_kwargs)
    norm_cls = partial(
        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs
    )
    if d_intermediate == 0:
        mlp_cls = nn.Identity
    else:
        mlp_cls = partial(
            GatedMLP, hidden_features=d_intermediate, out_features=d_model, **factory_kwargs
        )

    block = HydraBlock(
        d_model,
        mixer_cls,
        mlp_cls,
        norm_cls=norm_cls,
        fused_add_norm=fused_add_norm,
        residual_in_fp32=residual_in_fp32,
    )

    block.layer_idx = layer_idx
    return block


def _init_weights(
    module,
    n_layer,
    initializer_range=0.02,  # Now only used for embedding layer.
    rescale_prenorm_residual=True,
    n_residuals_per_layer=1,  # Change to 2 if we have MLP
):
    '''
    Function from https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py
    '''
    if isinstance(module, nn.Linear):
        if module.bias is not None:
            if not getattr(module.bias, "_no_reinit", False):
                nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Embedding):
        nn.init.normal_(module.weight, std=initializer_range)

    if rescale_prenorm_residual:
        for name, p in module.named_parameters():
            if name in ["out_proj.weight", "fc2.weight"]:
                nn.init.kaiming_uniform_(p, a=math.sqrt(5))
                with torch.no_grad():
                    p /= math.sqrt(n_residuals_per_layer * n_layer)


class HydraBackbone(nn.Module):
    '''
    Backbone blocks of the Hydra Model.
    '''
    def __init__(self,
                d_model: int,
                num_layers: int,
                ssm_config=None,
                norm_epsilon: float = 1e-5,
                rms_norm: bool = False,
                initializer_cfg=None,
                fused_add_norm=False,
                residual_in_fp32=False,
                device=None,
                dtype=None,
                attn_layer_idx=[],
                attn_config={},
                d_intermediate=0
                ):
        
        factory_kwargs = {"device": device, "dtype": dtype}
        super().__init__()
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm

        self.blocks = nn.ModuleList(
            [
                create_block(
                    d_model,
                    d_intermediate=d_intermediate,
                    ssm_cfg=ssm_config,
                    attn_layer_idx=attn_layer_idx,
                    attn_cfg=attn_config,
                    norm_epsilon=norm_epsilon,
                    rms_norm=rms_norm,
                    residual_in_fp32=residual_in_fp32,
                    fused_add_norm=fused_add_norm,
                    layer_idx=i,
                    **factory_kwargs,
                )
                for i in range(num_layers)
            ]
        )

        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(
            d_model, eps=norm_epsilon, **factory_kwargs
        )

        self.apply(
            partial(
                _init_weights,
                n_layer=num_layers,
                **(initializer_cfg if initializer_cfg is not None else {}),
                n_residuals_per_layer=1 if d_intermediate == 0 else 2,  # 2 if we have MLP
            )
        )


    def forward(
        self,
        x,
        inference_parameters=None
    ):
            
        hidden_states = x

        residual = None

        #for block in self.blocks: hidden_states, residual = block(hidden_states, residual, inference_params=inference_parameters)
        for block in self.blocks: hidden_states, residual = block(hidden_states, residual)

        if not self.fused_add_norm:
            residual = (hidden_states + residual) if residual is not None else hidden_states
            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))
        else:
            # Set prenorm=False here since we don't need the residual
            hidden_states = layer_norm_fn(
                hidden_states,
                self.norm_f.weight,
                self.norm_f.bias,
                eps=self.norm_f.eps,
                residual=residual,
                prenorm=False,
                residual_in_fp32=self.residual_in_fp32,
                is_rms_norm=isinstance(self.norm_f, RMSNorm)
            )

        return hidden_states


class SimpleCrossAttention(nn.Module):
    def __init__(self, dim, n_heads=4):
        super().__init__()
        self.n_heads = n_heads
        self.dim = dim
        self.head_dim = dim // n_heads
        assert self.head_dim * n_heads == dim, "dim must be divisible by n_heads"

        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)

    def forward(self, query, context):
        # query: (B, Nq, D)
        # context: (B, Nc, D)
        B, Nq, D = query.shape
        Nc = context.shape[1]

        # linear projections
        Q = self.q_proj(query).view(B, Nq, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, Nq, Hd)
        K = self.k_proj(context).view(B, Nc, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, Nc, Hd)
        V = self.v_proj(context).view(B, Nc, self.n_heads, self.head_dim).transpose(1, 2)  # (B, H, Nc, Hd)

        # scaled dot-product attention
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, H, Nq, Nc)
        attn_weights = torch.softmax(attn_scores, dim=-1)
        context_info = torch.matmul(attn_weights, V)  # (B, H, Nq, Hd)

        # merge heads
        context_info = context_info.transpose(1, 2).contiguous().view(B, Nq, D)
        return self.out_proj(context_info) + query  # residual connection



class HydraModel(nn.Module):
    '''
    Actual full Hydra Model used.
    '''

    def __init__(self,
                 encoder,
                 n_out,
                 ninp,
                 nhid,
                 num_layers: int = 1,
                 ssm_config = None,
                 norm_epsilon: float = 1e-5,
                 rms_norm: bool = False,
                 y_encoder=None,
                 use_cross_attention=False,
                 initializer_config = None,
                 fused_add_norm = False,
                 residual_in_fp32 = False,
                 device = "cpu",
                 num_permutations: int = 1,
                 dtype=None
                 ) -> None:

        super().__init__()
        self.model_type = 'mamba-ssm'
        self.encoder = encoder
        self.num_layers = num_layers
        self.device = device
        self.dtype = dtype
        self.ssm_config = ssm_config
        self.ssm_config = {"layer": "Hydra"}       # Specify the mamba version used
        self.rms_norm = rms_norm
        self.y_encoder = y_encoder
        self.initializer_config = initializer_config
        self.residual_in_fp32 = residual_in_fp32
        self.fused_add_norm = fused_add_norm
        self.cross_attn = SimpleCrossAttention(dim=ninp, n_heads=4)
        print(f"Hydra with cross attention set to {use_cross_attention}")
        self.use_cross_attention = use_cross_attention
        self.num_permutations = num_permutations

        factory_kwargs = {"device": device, "dtype": dtype}

        self.mamba_backbone = HydraBackbone(
            d_model=ninp,
            num_layers=self.num_layers,
            ssm_config=self.ssm_config,
            norm_epsilon=1e-5,
            rms_norm=False,     # Doesn't work with true yet.
            initializer_cfg=self.initializer_config,
            fused_add_norm=self.fused_add_norm,
            residual_in_fp32=self.residual_in_fp32,
            device=self.device,
            dtype=self.dtype
        )

        self.decoder = nn.Sequential(nn.Linear(ninp, nhid), nn.GELU(), nn.Linear(nhid, n_out))

        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(
            ninp, eps=norm_epsilon, **factory_kwargs
        )

        self.apply(
            partial(
                _init_weights,
                n_layer=num_layers,
                **(initializer_config if initializer_config is not None else {}),
            )
        )

    def permute_context(self, x_src, y_src, single_eval_pos):

        device = x_src.device
        perm = torch.randperm(single_eval_pos, device=device)

        x_perm = x_src.clone()
        y_perm = y_src.clone()

        x_perm[:single_eval_pos] = x_perm[perm]
        y_perm[:single_eval_pos] = y_perm[perm]

        return x_perm, y_perm


    def forward(
        self,
        src: tuple,
        single_eval_pos: int,
        compute_perm_reg: bool = False,
        **kwargs
    ):

        _, x_src, y_src = src

        # Encode
        x_src = self.encoder(x_src)
        y_src = self.y_encoder(
            y_src.unsqueeze(-1) if len(y_src.shape) < len(x_src.shape) else y_src
        )

        # ---------- ORIGINAL CONTEXT ----------
        context_tokens = x_src[:single_eval_pos] + y_src[:single_eval_pos]   # (S, B, D)
        query_tokens = x_src[single_eval_pos:]                              # (Sq, B, D)

        # Move to (B, S, D)
        context_tokens = context_tokens.permute(1, 0, 2)
        query_tokens = query_tokens.permute(1, 0, 2)

        B, S, D = context_tokens.shape
        P = self.num_permutations

        if P > 1:
            # Create P random permutations
            perms = torch.stack(
                [torch.randperm(S, device=context_tokens.device) for _ in range(P)],
                dim=0
            )  # (P, S)

            # Expand context to (P, B, S, D)
            context_expanded = context_tokens.unsqueeze(0).expand(P, B, S, D)

            # Apply permutations
            perms_expanded = perms.unsqueeze(1).unsqueeze(-1).expand(P, B, S, D)
            context_permuted = torch.gather(context_expanded, 2, perms_expanded)

            # Merge permutation and batch dimension
            context_permuted = context_permuted.reshape(P * B, S, D)

            # Run backbone once
            hidden = self.mamba_backbone(context_permuted, inference_parameters=None)

            # Reshape back to (P, B, S, D)
            hidden = hidden.reshape(P, B, S, D)

            # Average over permutations
            context_hidden = hidden.mean(dim=0)   # (B, S, D)

        else:
            context_hidden = self.mamba_backbone(
                context_tokens,
                inference_parameters=None,
            )

        # ---------- Cross Attention ----------
        if self.use_cross_attention:
            conditioned_query = self.cross_attn(query_tokens, context_hidden)
            conditioned_query = conditioned_query.permute(1, 0, 2)
            output = self.decoder(conditioned_query)
        else:
            full_sequence = torch.cat([context_hidden, query_tokens], dim=1)
            full_sequence = full_sequence.permute(1, 0, 2)
            decoded = self.decoder(full_sequence)
            output = decoded[single_eval_pos:]

        # ---------- PERMUTED CONTEXT (for regularization) ----------
        perm_context_hidden = None
        if compute_perm_reg:
            x_perm, y_perm = self.permute_context(x_src, y_src, single_eval_pos)

            context_tokens_perm = x_perm[:single_eval_pos] + y_perm[:single_eval_pos]
            context_tokens_perm = context_tokens_perm.permute(1, 0, 2)

            perm_context_hidden = self.mamba_backbone(
                context_tokens_perm, inference_parameters=None
            )

        return output, context_hidden, perm_context_hidden

